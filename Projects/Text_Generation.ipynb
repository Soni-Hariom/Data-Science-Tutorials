{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b87178",
   "metadata": {},
   "source": [
    "## Task : Text Generation\n",
    "\n",
    "**Description:** You are given a dataset of tweets from a particular user. Your task is to build a machine learning model to generate new tweets based on the user's existing tweets.\n",
    "\n",
    "**Dataset:** \"Sentiment140 dataset\" available on [Kaggle](https://www.kaggle.com/kazanova/sentiment140)\n",
    "\n",
    "**Requirements:**\n",
    "* Use Python programming language to build your model\n",
    "* Use natural language processing techniques to preprocess the data (e.g. remove stop words, punctuation, and perform stemming/lemmatization)\n",
    "* Use a machine learning algorithm of your choice to train the model\n",
    "* Implement a text generation algorithm that takes a seed sentence as input and generates a new tweet based on the patterns learned from the existing tweets\n",
    "* Evaluate the performance of the text generation algorithm using human evaluation and/or automatic evaluation metrics (e.g. perplexity, BLEU score, or ROUGE score)\n",
    "\n",
    "**Deliverables:**\n",
    "* A Jupyter notebook with your Python code and the results of your analysis\n",
    "* A brief report explaining your approach and discussing the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8828e157",
   "metadata": {},
   "source": [
    "## Step-0: Installing The Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c87e670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: opendatasets in /opt/conda/lib/python3.9/site-packages (0.1.22)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from opendatasets) (4.62.3)\n",
      "Requirement already satisfied: kaggle in /opt/conda/lib/python3.9/site-packages (from opendatasets) (1.5.13)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from opendatasets) (8.0.3)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.9/site-packages (from kaggle->opendatasets) (2.8.2)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.9/site-packages (from kaggle->opendatasets) (8.0.1)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.9/site-packages (from kaggle->opendatasets) (1.16.0)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.9/site-packages (from kaggle->opendatasets) (1.26.7)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.9/site-packages (from kaggle->opendatasets) (2021.10.8)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from kaggle->opendatasets) (2.26.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.9/site-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->kaggle->opendatasets) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->kaggle->opendatasets) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk;\n",
    "!pip install opendatasets;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b1a991",
   "metadata": {},
   "source": [
    "## Step-1: Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63b536fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "import opendatasets as od\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb48924c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the necessary NLTK data (only needs to be done once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eccf871",
   "metadata": {},
   "source": [
    "## Step-2: Loding The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bdf668a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./sentiment140\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "# Downloding the dataset:\n",
    "data=od.download('https://www.kaggle.com/datasets/kazanova/sentiment140')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c742dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the datasets\n",
    "df=pd.read_csv('sentiment140/training.1600000.processed.noemoticon.csv', header=None, names=['target', 'ids', 'datetime', 'flag', 'user','tweet'], encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c6207c",
   "metadata": {},
   "source": [
    "## Step-3: Viewing The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c5abae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>datetime</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                      datetime      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                              tweet  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c4fa024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the missing values\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "625913d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's count the positive and negative tweets\n",
    "df['target']=df['target'].replace({4:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b25cb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    800000\n",
       "1    800000\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a80b4",
   "metadata": {},
   "source": [
    "### Conclusion: \n",
    "* Here we can see the tweets are balanced.\n",
    "* 80k tweets are positive (1) and 80k tweets are negative (0).\n",
    "\n",
    "### Decisions:\n",
    "* Since our task is to build a *Text Generation Model*. So I'm Just going to take the tweets column.\n",
    "* Now in the tweet column there is texts so I need to preprocess them.\n",
    "* But it's 1.6 Million tweets so I am going to build a function to do all the needed steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f499c266",
   "metadata": {},
   "source": [
    "## Step-4: Tweets Preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7e683da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the set of stop words\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69d39fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stemmer and lemmatizer object\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eb0fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess a single tweet\n",
    "def preprocess_tweet(tweet):\n",
    "    # Remove URLs, user mentions, and special characters from the tweet\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+|@\\S+|[^\\w\\s]\", '', tweet)\n",
    "    # Tokenize the tweet into words\n",
    "    words = word_tokenize(tweet)\n",
    "    # Remove stop words and punctuation\n",
    "    words = [word.lower() for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "    # Perform stemming or lemmatization\n",
    "    words = [stemmer.stem(word) for word in words]  # or lemmatizer.lemmatize(word) for word in words]\n",
    "    # Join the words back into a single string\n",
    "    tweet = ' '.join(words)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87362ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to each tweet\n",
    "df['processed_tweet'] = df['tweet'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d54c3cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>datetime</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>that bummer shoulda got david carr third day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>upset cant updat facebook text might cri resul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>dive mani time ball manag save rest go bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>whole bodi feel itchi like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>behav im mad cant see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                      datetime      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                              tweet  \\\n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                     processed_tweet  \n",
       "0       that bummer shoulda got david carr third day  \n",
       "1  upset cant updat facebook text might cri resul...  \n",
       "2       dive mani time ball manag save rest go bound  \n",
       "3                    whole bodi feel itchi like fire  \n",
       "4                              behav im mad cant see  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the preprocessed dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7b50d",
   "metadata": {},
   "source": [
    "## Step-5: Data Exploration and Visualization\n",
    "After pre-processing your tweet dataset, you can use various data exploration and visualization techniques to gain insights into the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74489466",
   "metadata": {},
   "source": [
    "#### 1. Frequency Analysis:\n",
    "Let's analyze the frequency of individual words or phrases in the dataset using Python's Counter object from the collections module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad61c825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the module\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a18bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Counter object to count the frequency of each word\n",
    "word_freq = Counter()\n",
    "for tweet in df['processed_tweet']:\n",
    "    word_freq.update(tweet.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f0b641c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('im', 177512), ('go', 137040), ('get', 109956), ('day', 101292), ('good', 90551), ('work', 85194), ('like', 82962), ('love', 81080), ('got', 70036), ('dont', 66932)]\n"
     ]
    }
   ],
   "source": [
    "# Print the 10 most common words\n",
    "print(word_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0efe758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a histogram of the word frequencies\n",
    "plt.hist(word_freq.values(), bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b70b6",
   "metadata": {},
   "source": [
    "#### 2. Topic Modeling:\n",
    "Let's use topic modeling techniques such as Latent Dirichlet Allocation (LDA) to identify the main topics in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67da612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "# import sklearn\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d336603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the preprocessed tweets using a CountVectorizer\n",
    "# vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "# tweet_vectors = vectorizer.fit_transform(df['processed_tweet'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "391946c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LDA model and fit it to the tweet vectors\n",
    "# lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "# lda_model.fit(tweet_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c1910cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 10 words for each topic\n",
    "# for i, topic in enumerate(lda_model.components_):\n",
    "#     print(f\"Topic {i+1}:\")\n",
    "#     print([vectorizer.get_feature_names()[index] for index in topic.argsort()[-10:]])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6551df1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
      "     |████████████████████████████████| 585.9 MB 3.7 MB/s eta 0:00:01  ██                   | 236.3 MB 56.6 MB/s eta 0:00:07 "
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "219a59a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_270/535381913.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3be4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['tweet'])\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738b26c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to sequences\n",
    "input_sequences = []\n",
    "for line in df['tweet']:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44524d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0c33f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split input and output\n",
    "xs = input_sequences[:, :-1]\n",
    "labels = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f14e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical\n",
    "ys = pd.get_dummies(labels).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee496999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb19af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(xs, ys, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5631488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new tweets\n",
    "seed_text = \"I love\"\n",
    "next_words = 10\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict_classes(token_list, verbose=0)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438249b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}